---
title: "Part 2: Regression analysis with time series data"
author: "JÃ¸rn I. Halvorsen"
#bibliography: bibliography.bib
#output:
#  xaringan::moon_reader:
#    lib_dir: libs
#    nature:
#      highlightStyle: github
#      highlightLines: true
#      countIncrementalSlides: false
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r, eval=F, include=F}
rmarkdown::render("learning_objectives.Rmd")
system('brave learning_objectives.html')
```

```{r, eval=T, include=F}
#library(MSB104EconometricsJIH)
#data(package='MSB104EconometricsJIH')
```
# 10: Basic regression analysis with time series data 

[Wooldridge 10](https://github.com/joernih/MSB104TimeSeries/blob/main/inst/docs/CH10_Wooldridge_7e%20PPT_2pp.pptx)

## Nature of time series data abc
- Process (temporal): t=1,2,...,n
- Stochastic: {}
- Variables: $(x_{t1},x_{t2},...,x_{tk},y_{t})$
- Together: $\{(x_{t1},x_{t1},...,x_{tk},y_{t});t=1,2,..,n\}$

## Examples of time series regression models
### Static models 
- Example model (1) Phillips-curve
	- $inf_{t}=\beta_{0} + \beta_{1}unem_{t} + u_{t}$

### Finite distributed lagged models
- General model
  - $y=\alpha+\delta_{0}z_{t}+\delta_{1}z_{t-1}+\delta_{t-2}z_{2}+...+\delta_{N}z_{t-N}+u_{t}$
- Example model (2a) Fertility
	- $gfr_{t}=\alpha_{0}+\delta_{0}pe_{t}+\delta_{1}+pe_{t-1} + \delta_{2}pe_{t-2}+u_{t}$
- Example model (2b) Covid-19
	- $deathn_{t}=\alpha_{0}+\delta_{0}icu_{t}+\delta_{1}+icu_{t-1} + \delta_{2}icu_{t-2}+u_{t}$
<!--- Example model (3a) AR(1)
	- $y_{t}=\rho y_{t-1} + e_{t}$
-->
<!--- Example model (3b) MA(1)
	- $y_{t}=\theta e_{t-1} + e_{t}$
-->

Temporary change by 1 in period t
Impact-propensity (general model)

- 0. periods: $\delta_{0}$
- 1. periods: $\delta_{1}$
- 2. periods: $\delta_{2}$
- 3. periods: $0$

Permanent change by 1 in period t
Long-run propensity (general model)

- 0. periods: $\delta_{0}$
- 1. periods: $\delta_{0}+\delta_{1}$
- 2. periods: $\delta_{0}+\delta_{1}+\delta_{2}$
- 3. periods: $\delta_{0}+\delta_{1}+\delta_{2}$

## Finite sample properties of OLS under classical assumptions (small sample)
$y_{t}=\beta_{0}+\beta_{1}x_{t1}+\beta_{2}x_{t1}+...+\beta_{k}x_{t1}+u_{t}$

- TS.1: Linear in parameters

- TS.2: No perfect collinearity

- TS.3: Zero conditional mean 
  - $E(u_{t}|\textbf{X})=0$ for $t=0,1,2,...,n$

- TS.4: No heteroskedasticity 
  - $Var(u_{t}|\textbf{X})=\sigma^2$ 

- TS.5: No serial correlation 
  - $Corr(u_{t},u_{s}|\textbf{X})=0 for s \neq t$                      

- TS.6: Normality                      
  - $u_{t}$ i.i.d. distributed  $\sim N(0,\sigma^2)$ 

1. TS.1, TS.2 og TS.3 $\Rightarrow$ Unbiasedness
1. TS.1, TS.2, TS.3, TS.4 og TS.5 $\Rightarrow$ OLS is BLUE
1. TS.1, TS.2, TS.3, TS.4, TS.5 and TS.6 $\Rightarrow$ OLS estimatiors for $\beta_{j} \text{ where } j=0,1,..,k$ are normaly distributed

## Functional form, dummy variables, and index numbers

### Functional form
  - $deathn_{t}=\alpha_{0}+\delta_{0}icu_{t}+\delta_{1}icu_{t-1} + \delta_{2}icu_{t-2}+u_{t}$
  - $log(deathn_{t})=\alpha_{0}+\delta_{0}log(icu_{t})+\delta_{1}log(icu_{t-1}) + \delta_{2}log(icu_{t-2})+u_{t}$

### Dummy variables
- $inf_{t}=\beta_{0} + \beta_{1}unem_{t} + \beta_{t}lock_{t} + u_{t}$
- $deathn_{t}=\alpha_{0}+\delta_{0}icu_{t}+\delta_{1}+icu_{t-1} + \delta_{2}icu_{t-2}+\delta_{3}lock_{t-1}+u_{t}$

### Index numbers
- $inf_{t}=\beta_{0} + \beta_{1}unem_{t} + \beta_{t}stringindex_{t} + u_{t}$
  
## Trends and seasonality
### Trends
- Modelling a linear time trend
	- $y_{t}=\alpha_{0}+\alpha_{1}t + e_{t}$   
	- $E(y_{t})=\alpha_{0}+\alpha_{1}t$
	- $E(y_{t}-y_{y-1})=\alpha_{1}(t-(t-1))=\alpha_{1}t$
- Modelling an exponential time trend
	- $log(y_{t})=\alpha_{0}+\alpha_{1}t+e_{t}$   
	- $E(log(y_{t}))=\alpha_{0}+\alpha_{1}t$
	- $E(log(y_{t}/y_{y-1}))=\alpha_{1}(t-(t-1))=\alpha_{1}t$

When to include a trend?

- If dependent variable displays obvious trending behaviour
- If some of the independent variables have trends
- If the dependent variable and some of the independent variables have trends

Estimating the model with a trend $\equiv$ Detrending (with the estimation coefficient) the dataset before estimation

Consequences of not including a trend:

- Yield bias OLS estimators if more than one variables are driven by an unobserved trending factors
	- Spurious regression: But the usual and adjusted R-squared for time series regressions can be artificially high when the dependent variable is trending.

### Seasonality
- Quarterly:
	- $y_{t}=\beta_{0}+ \alpha_{1}y_{t_1} +\delta_{1}aut_{t}+\delta_{2}sum_{t}+ \delta_{3}spr_{t}+u_{t}$

# 11: Further issues in using OLS with time series data 

[Wooldridge 11](https://github.com/joernih/MSB104TimeSeries/blob/main/inst/docs/CH11_Wooldridge_7e%20PPT_2pp.pptx)

## Stationarity and weekly dependent time series 

### Stationarity
#### Stationarity
A stochastical process $\{x_{t};t=1,2,..,n\}$ is stationary if for every collection of time indeces
$1\leq t_{1}<t_{1}<..>t_{m}$
The joint distribution of
$(x_{t_{1}}, x_{t_{2}},...,x_{t_{n}})$
is the same as
$(x_{t_{1+h}}, x_{t_{2+h}},...,x_{t_{n+h}})$
for all $h \geq 1$

### Covariance stationarity
A stochastical process $\{x_{t};t=1,2,..,n\}$ is covariance stationarity if its 

- Expected value constant over time, $E(x_t)=\mu$
- Variance constant over time, $Var(x_t)=\sigma$
- Covariance only depends on $h$, $Cov(x_{t+h},x_t)=f(h)$

### Weakly dependency (I(0) - integrated of order zero)
A stationary time series $\{x_{t};t=1,2,..,n\}$ is weekly dependent if $x_{t}$ and $x_{t+h}$ are almost independent as h increases without bond.

A covariance stationary process is weekly dependent if the correlation between $x_{t}$ and $x_{t+h}$ goes to zero 'sufficient quickly' as $h \rightarrow \infty$.

Examples:

- MA(1) process $y_{t}=e_{t}+\alpha_{1}e_{t-1}$
  - The process is weekly dependent because observations that are more than one time period apart have nothing in common and therefore uncorrelated. 
- AR(1) process $y_{t}=\rho_{1}y_{t-1}+e_{t}$
  - $\Rightarrow Corr(y_{t},y_{t+h})=\rho_{1}^{h}$
  - Stable process ($|\rho|<1$) is weekly dependent

<!-- Hybrid AR(1) and MA(1), ARMA(1) -->

## Asymptotic properties of OLS   
- TS'.1: Linear in parameters and that 
  $\{ (y_{t},{\bf{x_{t}}}); t=1,2,...,n  \}); t=1,2,...,n  \}$ are stationary and weekly dependent
  $\Rightarrow$ we can apply LLN (law of large numbers) and CLT (central limit teorem)

- TS'.2=TS.2: No perfect collinearity

- TS'.3: Weakly exogenous
  - $E(u_{t}|{\bf{}x_{t}})=0$ for $x_{t}=(x_{t1},x_{t2},...,x_{tk})$

- TS'.4: $Var(u_{t}|{\bf{}x_{t}})=\sigma^2$ contemporanous heteroskedasticity

- TS'.5: $Corr(u_{t},u_{s}|{\bf{}x_{t}},{\bf{}x_{s}})=0$ for $s \neq t$ No serial correlation

1. TS'.1, TS'.2 og TS'.3 $\Rightarrow plim \hat{\beta_{j}}=\beta_{j} \text{ for} j=0,1,2,...,k$ are consistent 

1. TS'.1, TS'.2, TS'.3, TS'.4 og TS'.5 $\Rightarrow$ OLS estimators asymptotically normally distributed $\Rightarrow$ OLS estimator, t-,F-,LM statics asympotically valid

## Using highly persistent time series (I(1) - integrated of order one) in regression analysis 

### Random walk
$y_{t}=e_{t}+e_{t-1}+...+e_{1}+y_{0}$

- Taking $E(y_{t})$ implies

$E(y_{t})=y_{0}$.
Does not depend on t (constant)

- Taking $Var(y_{t})$ implies

$Var(y_{t})=t\sigma^2_{e}$.
Increases as a linear function over time $\Rightarrow$ not stationary

- Forecasting period t+h ahead

$E(y_{t+h}|y_{t})=y_{t}$.
Regardless of how far in the future we want to predict, today's value is our best predictior

- Finding $Corr(y_{t+h},y_{t})$ implies

$Corr(y_{t+h},y_{t})=\sqrt{t/(t+h)}$.
Dependent on t (not constant)

### Random walk with drift

$y_{t}=\alpha_{0} t + \alpha_{1t}+e_{t-1}+...+e_{1}+y_{0}$

- Taking $E(y_{t})$ implies

$E(y_{t})=\alpha_{0}t+y_{0}$

- Taking $Var(y_{t})$  

$Var(y_{t})=t\sigma^2_{e}$.
same result as under RW

- Forecasting t+h periods ahead 

$E(y_{t+h}|y_{t})=\alpha_{0}h+y_{t}$

- Finding $Corr(y_{t+h},y_{t})$ implies

$Corr(y_{t+h},y_{t})=\sqrt{t/(t+h)}$.
Dependent on t (not constant)

### Differencing a highly persistent time series

- We can use the value of $\hat{\rho}$ (first-order autocorrelation of y_{t}) to help decide whether the process is I(1) or I(0).

- Formal tests exist (E.g. [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test)), but not part of this curriculum

- Both unit root and trend may be elimaniting by differencing:
$\Delta y_{t}=y_{t}-y_{t-1}=e_{t}$

## Dynamically complete models and the absence of serial correlation
A model is said to be dynamically complete if enough lagged variables have been included as explanatory variables so that further lags do not help to explain the dependent variable:
$E(y_{t}|{\bf{x}_{t}}, {\bf{x}_{t-1}}, y_{t-1},...) = E(y_{t}|{\bf{x}}_{t})$

- So once the explanatory variables ${\bf{x}_{t}}$ in the model is controlled for, no further lags help to explain the current z
- Dynamic completness implies absence of serial correlation
- Sequential exogenity implies that causal effects will correctly estimated, even with the absence of serial correlation

## The homoskedasticity assumption for time series models

$Var(u_{t}|{\bf{x_{t}}}) = Var(y_{t}|{\bf{x_{t}}}) = \sigma^{2}$

So even though $y_{t}$ is a linear function of ${\bf{x_{t}}}$, the $Var(y_{t}|{\bf{x_{t}}})$ must be constant.

# 12: Serial correlation and heteroskedasticity in time series regression 
[Wooldridge 12](https://github.com/joernih/MSB104TimeSeries/blob/main/inst/docs/CH12_Wooldridge_7e%20PPT_2pp.pptx)

## Properties of OLS under serial correlated errors 
- Given that TS.1/TS'.1, TS.2/TS'.2 and TS.3/TS'3 still hold, OLS still unbiased/consistent $\Rightarrow$ point estimates and $R^{2}$ valid 
- Serial correlation violation of TS.5/TS' $\Rightarrow$
	- OLS inefficient (no longer BLUE) 
	- Standard errors (typically underestimated), t-,F and LM- statistics not valid

## Testing for serial correlation 
### With strictly exogenous regressors
#### t-test
- $H_{0}:\rho=0$ No serial correlation
- $H_{1}:\rho=0$ Serial correlation
- Assume that
	- $E(e_{t}|u_{t}, u_{t-1}, u_{t-2})=0$ 
	- $Var(e_{t}|u_{t}, u_{t-1}, u_{t-2})=Var(e_{t})=\sigma^{2}_{e}$

#### Durbin-Watson test
- T-test is commonly considered to be better, so we will only consider t-test in this course.

### Without strictly exogenous regressors (General Breusch-Godfrey test for AR(q) serial correlation
- (i) Run the OLS regression of $y_{t}$ on $x_{t1},...,x_{tk}$ and obtain the OLS residuals $\hat{u}_{t}$
- (ii) Run the regression of $\hat{u}_{t}$ on $x_{t1},...,x_{tk},$\hat{u}_{t-1} for all $t=2,...,n$ to obtain $\hat{\rho}$ on $\hat{u}_{t-1}$ 
- (iii) Test $H_{0}:\rho=0$ vs. $H_{1}:\rho\neq0$


### Testing for higher order serial correlation
- Se Woodford for AR(2)
- Harvey (1990) provides a general approach


## Correcting for serial correlation with strictly exogenous regressors

If we detect serial correlation after applying some of the tests, we have to do something about it:

- Goal: model with complete dynamics $\Rightarrow$ need to respecify the model. Ex. FGLS (either Cochrane-Orcutt or Prais Winsten)

- For model with incomplete dynamics $\Rightarrow$ find a way to carry out statistical inference. Ex. Serial Correlation-Robust  Inference after OLS

## Differencing and serial correlation

For $y_{t}=\beta_{0}+\beta_{1}x_{t}+u_{t}$. We should differentiate 
if
  
- If $u_{t}$ follows a random walk, differencing will imply that $\delta u_{t}$ is serially uncorrelated
- If $u_{t}$ do not follow a random walk, but the estimation of $\rho$ on its lagged value is high, differencing
  will eliminate most of the autocorrelation.

## Serial correlation-robust inference after OLS

- One can compute serial correlation-robust standard errors after OLS (Newey-West Standard Errors)
- HAC (heteroskedasticity and autocorrelation consistent) standard errors
	- (i) Estimate model by OLS, which yields $\text{"}se(\hat{\beta_{j}})\text{"}$, $\hat{\sigma}$, $\hat{u}_{t}:t=1,...,n$.
	- (ii) Compute the residuals $\hat{r}_{t}:t=1,...,n$, then form $\hat{a}_{t}=\hat{u}_{t}\hat{r}_{t}$
	- (iii) For your choice of $g$, compute $\hat{v}$
	- (iv) Compute $se(\hat{\beta_{j}})=\left[\frac{\text{"}se(\hat{\beta_{j}})\text{"}}{\hat{\sigma}} \right ]^{2}\sqrt{v}$
- An implementation in R can be found [here](https://www.r-econometrics.com/methods/hcrobusterrors/)

## Heteroskedasticity in time series regression

In recent years, it has become more popular to estimate models by OLS but to correct the standard errors for fairly arbitrary forms of serial correlation (and heteroskedasticity).

### Correction

- Receive less attention than serial correlation
- Heteroskedasticity-robust standard errors also work for time series
	- serial correlation-robust formulas for standard errors and test statistics

### Testing

- Heteroskedasticity test assumes the absence of serial correlation $\Rightarrow$
	- Test for serial correlation (heteroskedasticy-robust test)
	- After potential serial correlation corrected for, test for heteroskedasticity

# 16: Simultanous equations models (SEM) 
[Wooldridge 16](https://github.com/joernih/MSB104TimeSeries/blob/main/inst/docs/CH16_Wooldridge_7e%20PPT_2pp.pptx)

## The nature of simultanous equations models

Endogeneity problem: 

- (1) Measurement error 
- (2) Omitted variables 
- (3) **Simultaneity: one or more of the explanatory variables are jointly determined with the dependent variable**

SEM model: each equation in the system should have a ceteris paribus, causal interpreation

Example: Supply and labor demand

Set of three equations:

- $h_{s}=\alpha_{1}w+\beta_{1}z_{1}+u_{1}$
- $h_{d}=\alpha_{2}w+\beta_{2}z_{2}+u_{2}$
- $h_{s}=h_{d}=h$

Which can be combined into a set of two equations of a 
simultanous equations model (SEM):

- $h_{i}=\alpha_{1}w_{i}+\beta_{1}z_{i,1}+u_{i,1}$
- $h_{i}=\alpha_{2}w_{i}+\beta_{2}z_{i,2}+u_{i,2}$

Endogenous:$h_{i},w_{i}$
Exogenous: $z_{i,1}, z_{i,2}$

## Simultaneity bias in OLS
- $y_{1}=\alpha_{1}y_{2}+\beta_{1}z_{1}+u_{1}$
- $y_{2}=\alpha_{2}y_{1}+\beta_{2}z_{2}+u_{2}$

Solve for the reduced form equation $y_{22}$
$y_{2}=\alpha_{2}(\alpha_{1}y_{2}+\beta_{1}z_{1}+u_{1})+\beta_{2}z_{2}+u_{2}$
$y_{2}(1-\alpha_{2}\alpha_{1})=\alpha_{2}\beta_{1}z_{1}+\beta_{2}z_{2}+\alpha_{2}u_{1}+u_{2}$

Assume that $\alpha_{2}\alpha_{1}\neq0$ 

$y_{2}=\pi_{21}z_{1}+\pi_{22}z_{2}+v_{2}$

Where the reduced form parameters and errors
- $\pi_{21}\equiv\frac{\alpha_{2}\beta_{1}}{(1-\alpha_{2}\alpha_{1})}$

$\alpha_{2} \neq 0 \rightarrow$ simultaneity bias

## Identifying and estimating a structural equations

OLS is biased and inconsistent when applied to an equation in SEM
2SLS can be applied to a SEM

Special case:

- $h=\alpha_{1}w+\beta_{1}z_{1}+u_{1}$
- $h=\alpha_{2}w+u_{2}$

Given a random sample of (q,p,z), which of the equations (identified equation) above can be estimated by 2SLS

z_{1} can be used as an IV-variable in the demand equation

## (System with more than two equations)
Not covered in this course

## (Simultanous equations models with time series)
Not covered in this course

## (Simultanous equations models with panel data)
Not covered in this course

```{r include=F}
knitr::knit_exit()
```


# Repetition time series data (2 hours)
## Data realization

## Data generation process

## Model estimation

## Properties

## Diagnostics

## Estimation results

Tasks:
Asymptotic
Eksamen 
PrÃ¸veeksamen

- Example model (3)
	- $y_{t}=\rho y_{t-1} + e_{t}$
- If $|\rho| < 1$ AR(1) process
	- $y_{t}=\rho y_{t-1} + e_{t}$
- If $\rho = 1$ Random walk
	- $y_{t}=\rho y_{t-1} + e_{t}$

Sequential exogenity

Example: AR(1) without an intercept:
- $y_{t}=\alpha_{0}+\rho y_{t-1} + e_{t}$

Example: AR(1) with an intercept:
- $y_{t}=\alpha_{0}+\rho y_{t-1} + e_{t}$

- If $\rho<|1|$ weekly dependence (no unit root, I(0)) -> TS'1. not violated
- If $\rho=1$ HP strongly dependence (unit root, I(1)) -> TS'1. violated 


# Introduction
# Stationarity and weekly dependent time series
# Asymptotic properties of OLS
# Using highly persistent timer series in regression analysis
# Complete dynamic models and the absence of serial correlation
# The homoskedasticity assumption for time series models






